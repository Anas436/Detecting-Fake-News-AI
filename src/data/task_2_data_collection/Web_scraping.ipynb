{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dffdc24",
   "metadata": {},
   "source": [
    " Web Scraping web application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "6c10c986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "819e1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to store the data\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "links = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "070e0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base URL\n",
    "base_url = \"https://tsmliberia.com/category/fact-check/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4d73217a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'article' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m date \u001b[38;5;241m=\u001b[39m post\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentry-date updated td-module-date\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     19\u001b[0m link \u001b[38;5;241m=\u001b[39m post\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, href\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m content \u001b[38;5;241m=\u001b[39m \u001b[43marticle\u001b[49m\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentry-content\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     24\u001b[0m titles\u001b[38;5;241m.\u001b[39mappend(title)\n\u001b[0;32m     25\u001b[0m authors\u001b[38;5;241m.\u001b[39mappend(author)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'article' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Scrape data from the first 20 pages\n",
    "for page in range(1, 40):\n",
    "    # Construct the URL for each page\n",
    "    url = base_url + f\"page/{page}/\"\n",
    "\n",
    "    # Send a GET request to the page\n",
    "    response = requests.get(url)\n",
    "    content = response.content\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Find the posts on the page\n",
    "    posts = soup.find_all(\"div\", class_=\"td_module_1 td_module_wrap td-animation-stack td-cpt-post\")\n",
    "\n",
    "    # Extract the information from each post and append it to the respective lists\n",
    "    for post in posts:\n",
    "        title = post.find(\"h3\", class_=\"entry-title td-module-title\").text.strip()\n",
    "        author = post.find(\"span\", class_=\"td-post-author-name\").text.strip()\n",
    "        date = post.find(\"time\", class_=\"entry-date updated td-module-date\").text.strip()\n",
    "        link = post.find('a', href= True).get('href')\n",
    "      \n",
    "      \n",
    "    \n",
    "\n",
    "        titles.append(title)\n",
    "        authors.append(author)\n",
    "        dates.append(date)\n",
    "        links.append(link)\n",
    "        contents.append(content)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a273de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pandas DataFrame using the collected data\n",
    "data = {\n",
    "    \"Title\": titles,\n",
    "    \"Author\": authors,\n",
    "    \"Date\": dates,\n",
    "    \"Link\": links,\n",
    "\n",
    "\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388c2fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Collected_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e3214",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected=pd.read_csv(\"Collected_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b01a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ded28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
